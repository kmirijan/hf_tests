{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Types of Basic Transformer Models\n",
    "- GPT-like: auto-regressive\n",
    "- BERT-like: auto-encoding\n",
    "- BART/T5-like: seq-to-seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Base models are pretrained, and usually need to be finetuned for specific tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Original concept for a transformer is divided into 2 parts; Encoder and Decoder\n",
    "- Encoder: Receives input and builds a representation of it as features. Thus, an encoder is optimized to acquire understanding from input\n",
    "- Decoder: Uses an encoder's feature representationalone with other inputs to generate a target sequence. Thus, a decoder is optimized for generating output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Encoder only: Good for tasks that require understanding of the input, such as sentence classification and NER\n",
    " - Decoder only: Good for generative tasks like text generation\n",
    " - Seq-to-Seq using both: Good for generative tasks that require input like translation and summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Attention\n",
    "The most important part of the transformer. The purpose of the attention layer is to allow the model to distinguish between important elements and pay specific <i>attention</i> to certain words and more or less ignore others"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoders\n",
    "The purpose of the encoder is to capture the meaning and context of words in a sequence in an <i>encoded</i> feature reperesentation in the form of a vector. It computes this representation using self-attention\n",
    "\n",
    "They are bi-directional and in training and testing needs to take in the entirety of the sequence. Encoder only models are thus good at extracting meaningful information. They end up being good at sequence classification, QA, and masked language modelling (guess the hidden word)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Funcionally, a decoder operates similarly to encoders. The main difference is that a decoder recives masked input. So at any point in a sequence, the decoder can only only register the context at that point in the sequence based on what came before. Not after. This make Decoders auto-regressive. They use their past \n",
    "\n",
    "Decoders are unidirectional and are great at text generation tasks. Thus, Decoder models are best used for causal language modelling or text sequence generation. What makes Decoder models auto-regressive is that they use their own output as context for future generation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoder-Decoder\n",
    "\n",
    "Also called sequence-to-sequence models. They use both parts of the transformer architecture.\n",
    "\n",
    "The encoder takes an input and returns an encoded feature representation. This feature representation is then delivered to the decoder. The decoder will then take a start of sequence word and it can begin decoding. With the encoded feature representation and it's own output, the decoder can continue to auto-regressively generate.\n",
    "\n",
    "Very common for translation modelling or summarization"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
